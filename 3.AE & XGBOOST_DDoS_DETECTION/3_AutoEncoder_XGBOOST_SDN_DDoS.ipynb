{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.AutoEncoder_XGBOOST_SDN_DDoS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gyjVSG5krSUL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/SDN_DDoS_.csv')"
      ],
      "metadata": {
        "id": "Qg7ETYUrrkMD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting dataset into features and label\n",
        "X= df.drop('Label', axis =1)\n",
        "y = df['Label']\n",
        "\n",
        "# Splitting the dataset into the training set and the test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "BlhJwLbNrpwu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale data\n",
        "t = MinMaxScaler()\n",
        "t.fit(X_train)\n",
        "X_train = t.transform(X_train)\n",
        "X_test = t.transform(X_test)"
      ],
      "metadata": {
        "id": "Z5yfIQEnrszx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AutoEncoder Model Preparation\n",
        "n_inputs = X.shape[1]\n",
        "# define encoder\n",
        "input_data_shape= Input(shape=(n_inputs,))\n",
        "# encoder level 1\n",
        "encoder= Dense(n_inputs*2)(input_data_shape)\n",
        "encoder = BatchNormalization()(encoder)\n",
        "encoder= LeakyReLU()(encoder)\n",
        "# encoder level 2\n",
        "encoder= Dense(n_inputs)(encoder)\n",
        "encoder= BatchNormalization()(encoder)\n",
        "encoder= LeakyReLU()(encoder)\n",
        "# bottleneck\n",
        "n_bottleneck = round(float(n_inputs) / 2.0)\n",
        "bottleneck = Dense(n_bottleneck)(encoder)\n",
        "# define decoder, level 1\n",
        "decoder = Dense(n_inputs)(bottleneck)\n",
        "decoder = BatchNormalization()(decoder)\n",
        "decoder = LeakyReLU()(decoder)\n",
        "# decoder level 2\n",
        "decoder = Dense(n_inputs*2)(decoder)\n",
        "decoder = BatchNormalization()(decoder)\n",
        "decoder = LeakyReLU()(decoder)"
      ],
      "metadata": {
        "id": "wQ2D-yxmrwvI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output layer\n",
        "output = Dense(n_inputs, activation='linear')(decoder)\n",
        "# define autoencoder model\n",
        "model = Model(inputs=input_data_shape, outputs=output)\n",
        "# compile autoencoder model\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "qWrqTrV5r1tY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FiN9FY0r5Sl",
        "outputId": "d8c8a0ff-a9c3-4a60-8e68-7fe20f8ff2a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 66)]              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 132)               8844      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 132)              528       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 132)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 66)                8778      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 66)               264       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 66)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 33)                2211      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 66)                2244      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 66)               264       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 66)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 132)               8844      \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 132)              528       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 132)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 66)                8778      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41,283\n",
            "Trainable params: 40,491\n",
            "Non-trainable params: 792\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the autoencoder model to reconstruct input\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=16, verbose=2, validation_data=(X_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhobjlyUsADo",
        "outputId": "24e5f946-d468-423c-c136-21e56c5fcbdb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3802/3802 - 12s - loss: 0.0104 - val_loss: 0.0063 - 12s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "3802/3802 - 10s - loss: 0.0020 - val_loss: 0.0035 - 10s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "3802/3802 - 11s - loss: 0.0015 - val_loss: 0.0026 - 11s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "3802/3802 - 10s - loss: 0.0012 - val_loss: 8.5620e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "3802/3802 - 10s - loss: 0.0011 - val_loss: 0.0030 - 10s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "3802/3802 - 10s - loss: 0.0010 - val_loss: 7.4517e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "3802/3802 - 10s - loss: 9.2583e-04 - val_loss: 0.0033 - 10s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "3802/3802 - 11s - loss: 9.1668e-04 - val_loss: 0.0023 - 11s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "3802/3802 - 11s - loss: 7.5791e-04 - val_loss: 0.0028 - 11s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "3802/3802 - 11s - loss: 8.1166e-04 - val_loss: 0.0138 - 11s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "3802/3802 - 11s - loss: 8.4282e-04 - val_loss: 0.0063 - 11s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "3802/3802 - 10s - loss: 7.5579e-04 - val_loss: 0.0016 - 10s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "3802/3802 - 11s - loss: 7.2147e-04 - val_loss: 0.0012 - 11s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "3802/3802 - 11s - loss: 6.9357e-04 - val_loss: 0.0075 - 11s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "3802/3802 - 10s - loss: 6.1931e-04 - val_loss: 8.8763e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "3802/3802 - 10s - loss: 6.6916e-04 - val_loss: 0.0026 - 10s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "3802/3802 - 10s - loss: 5.4262e-04 - val_loss: 6.3750e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "3802/3802 - 10s - loss: 6.4456e-04 - val_loss: 5.3886e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "3802/3802 - 10s - loss: 6.1213e-04 - val_loss: 0.0114 - 10s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "3802/3802 - 11s - loss: 5.8124e-04 - val_loss: 0.0024 - 11s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "3802/3802 - 11s - loss: 6.4323e-04 - val_loss: 0.0056 - 11s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "3802/3802 - 10s - loss: 5.3781e-04 - val_loss: 0.0020 - 10s/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "3802/3802 - 11s - loss: 5.4461e-04 - val_loss: 5.2246e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 24/100\n",
            "3802/3802 - 10s - loss: 5.4463e-04 - val_loss: 5.0205e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 25/100\n",
            "3802/3802 - 11s - loss: 4.2613e-04 - val_loss: 0.0010 - 11s/epoch - 3ms/step\n",
            "Epoch 26/100\n",
            "3802/3802 - 10s - loss: 6.4387e-04 - val_loss: 6.5844e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 27/100\n",
            "3802/3802 - 11s - loss: 4.9089e-04 - val_loss: 0.0077 - 11s/epoch - 3ms/step\n",
            "Epoch 28/100\n",
            "3802/3802 - 11s - loss: 4.3648e-04 - val_loss: 3.6184e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 29/100\n",
            "3802/3802 - 10s - loss: 5.0527e-04 - val_loss: 8.3390e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 30/100\n",
            "3802/3802 - 11s - loss: 4.2982e-04 - val_loss: 6.0945e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 31/100\n",
            "3802/3802 - 10s - loss: 3.9335e-04 - val_loss: 0.0030 - 10s/epoch - 3ms/step\n",
            "Epoch 32/100\n",
            "3802/3802 - 11s - loss: 4.4562e-04 - val_loss: 5.9312e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 33/100\n",
            "3802/3802 - 11s - loss: 3.9830e-04 - val_loss: 7.0197e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 34/100\n",
            "3802/3802 - 11s - loss: 3.5346e-04 - val_loss: 6.5082e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 35/100\n",
            "3802/3802 - 11s - loss: 3.4554e-04 - val_loss: 0.0062 - 11s/epoch - 3ms/step\n",
            "Epoch 36/100\n",
            "3802/3802 - 11s - loss: 3.4574e-04 - val_loss: 6.2589e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 37/100\n",
            "3802/3802 - 11s - loss: 3.6511e-04 - val_loss: 8.5525e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 38/100\n",
            "3802/3802 - 11s - loss: 3.9526e-04 - val_loss: 2.9296e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 39/100\n",
            "3802/3802 - 12s - loss: 3.3514e-04 - val_loss: 0.0011 - 12s/epoch - 3ms/step\n",
            "Epoch 40/100\n",
            "3802/3802 - 12s - loss: 3.9155e-04 - val_loss: 0.0082 - 12s/epoch - 3ms/step\n",
            "Epoch 41/100\n",
            "3802/3802 - 12s - loss: 3.1170e-04 - val_loss: 6.3697e-04 - 12s/epoch - 3ms/step\n",
            "Epoch 42/100\n",
            "3802/3802 - 12s - loss: 2.5418e-04 - val_loss: 6.5660e-04 - 12s/epoch - 3ms/step\n",
            "Epoch 43/100\n",
            "3802/3802 - 12s - loss: 3.0729e-04 - val_loss: 5.4351e-04 - 12s/epoch - 3ms/step\n",
            "Epoch 44/100\n",
            "3802/3802 - 11s - loss: 2.2525e-04 - val_loss: 7.1183e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 45/100\n",
            "3802/3802 - 11s - loss: 3.1354e-04 - val_loss: 0.0033 - 11s/epoch - 3ms/step\n",
            "Epoch 46/100\n",
            "3802/3802 - 11s - loss: 3.0393e-04 - val_loss: 7.5773e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 47/100\n",
            "3802/3802 - 11s - loss: 3.0600e-04 - val_loss: 4.1182e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 48/100\n",
            "3802/3802 - 11s - loss: 2.6437e-04 - val_loss: 5.9876e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 49/100\n",
            "3802/3802 - 11s - loss: 2.9173e-04 - val_loss: 3.7130e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 50/100\n",
            "3802/3802 - 11s - loss: 1.8758e-04 - val_loss: 6.5812e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 51/100\n",
            "3802/3802 - 11s - loss: 2.3552e-04 - val_loss: 5.0049e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 52/100\n",
            "3802/3802 - 10s - loss: 2.5453e-04 - val_loss: 6.0064e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 53/100\n",
            "3802/3802 - 11s - loss: 2.4335e-04 - val_loss: 6.7262e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 54/100\n",
            "3802/3802 - 11s - loss: 2.1117e-04 - val_loss: 0.0121 - 11s/epoch - 3ms/step\n",
            "Epoch 55/100\n",
            "3802/3802 - 10s - loss: 2.6297e-04 - val_loss: 6.1978e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 56/100\n",
            "3802/3802 - 11s - loss: 2.1337e-04 - val_loss: 8.1065e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 57/100\n",
            "3802/3802 - 11s - loss: 2.0307e-04 - val_loss: 6.8593e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 58/100\n",
            "3802/3802 - 11s - loss: 2.0540e-04 - val_loss: 8.4465e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 59/100\n",
            "3802/3802 - 11s - loss: 2.1244e-04 - val_loss: 7.3408e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 60/100\n",
            "3802/3802 - 11s - loss: 2.1193e-04 - val_loss: 6.4989e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 61/100\n",
            "3802/3802 - 10s - loss: 1.7694e-04 - val_loss: 6.3817e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 62/100\n",
            "3802/3802 - 11s - loss: 1.7619e-04 - val_loss: 0.0060 - 11s/epoch - 3ms/step\n",
            "Epoch 63/100\n",
            "3802/3802 - 11s - loss: 2.8913e-04 - val_loss: 0.0014 - 11s/epoch - 3ms/step\n",
            "Epoch 64/100\n",
            "3802/3802 - 11s - loss: 1.3191e-04 - val_loss: 6.3345e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 65/100\n",
            "3802/3802 - 11s - loss: 1.6415e-04 - val_loss: 6.1124e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 66/100\n",
            "3802/3802 - 11s - loss: 2.2088e-04 - val_loss: 0.0031 - 11s/epoch - 3ms/step\n",
            "Epoch 67/100\n",
            "3802/3802 - 10s - loss: 1.5625e-04 - val_loss: 0.0010 - 10s/epoch - 3ms/step\n",
            "Epoch 68/100\n",
            "3802/3802 - 11s - loss: 1.9613e-04 - val_loss: 9.2277e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 69/100\n",
            "3802/3802 - 11s - loss: 1.6820e-04 - val_loss: 7.4490e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 70/100\n",
            "3802/3802 - 11s - loss: 2.2315e-04 - val_loss: 7.2195e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 71/100\n",
            "3802/3802 - 11s - loss: 1.9955e-04 - val_loss: 7.5115e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 72/100\n",
            "3802/3802 - 10s - loss: 1.7640e-04 - val_loss: 6.6507e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 73/100\n",
            "3802/3802 - 11s - loss: 1.5097e-04 - val_loss: 6.1739e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 74/100\n",
            "3802/3802 - 10s - loss: 1.2466e-04 - val_loss: 8.0289e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 75/100\n",
            "3802/3802 - 11s - loss: 1.2532e-04 - val_loss: 9.3148e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 76/100\n",
            "3802/3802 - 11s - loss: 2.3296e-04 - val_loss: 5.5499e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 77/100\n",
            "3802/3802 - 11s - loss: 1.6846e-04 - val_loss: 8.0853e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 78/100\n",
            "3802/3802 - 10s - loss: 2.0552e-04 - val_loss: 2.9493e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 79/100\n",
            "3802/3802 - 10s - loss: 1.9249e-04 - val_loss: 3.6369e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 80/100\n",
            "3802/3802 - 10s - loss: 1.5481e-04 - val_loss: 7.1519e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 81/100\n",
            "3802/3802 - 11s - loss: 1.4743e-04 - val_loss: 8.3820e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 82/100\n",
            "3802/3802 - 10s - loss: 1.6440e-04 - val_loss: 0.0012 - 10s/epoch - 3ms/step\n",
            "Epoch 83/100\n",
            "3802/3802 - 10s - loss: 1.6820e-04 - val_loss: 4.1108e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 84/100\n",
            "3802/3802 - 11s - loss: 2.1986e-04 - val_loss: 0.0018 - 11s/epoch - 3ms/step\n",
            "Epoch 85/100\n",
            "3802/3802 - 10s - loss: 1.5675e-04 - val_loss: 0.0029 - 10s/epoch - 3ms/step\n",
            "Epoch 86/100\n",
            "3802/3802 - 10s - loss: 1.3213e-04 - val_loss: 0.0011 - 10s/epoch - 3ms/step\n",
            "Epoch 87/100\n",
            "3802/3802 - 11s - loss: 2.1554e-04 - val_loss: 4.7409e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 88/100\n",
            "3802/3802 - 11s - loss: 1.4872e-04 - val_loss: 4.7926e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 89/100\n",
            "3802/3802 - 11s - loss: 2.1318e-04 - val_loss: 3.4333e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 90/100\n",
            "3802/3802 - 10s - loss: 1.4490e-04 - val_loss: 5.6028e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 91/100\n",
            "3802/3802 - 11s - loss: 1.4984e-04 - val_loss: 5.6425e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 92/100\n",
            "3802/3802 - 11s - loss: 1.2632e-04 - val_loss: 6.1842e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 93/100\n",
            "3802/3802 - 11s - loss: 1.7509e-04 - val_loss: 5.9471e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 94/100\n",
            "3802/3802 - 11s - loss: 1.8509e-04 - val_loss: 8.8566e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 95/100\n",
            "3802/3802 - 10s - loss: 1.0618e-04 - val_loss: 0.0241 - 10s/epoch - 3ms/step\n",
            "Epoch 96/100\n",
            "3802/3802 - 11s - loss: 1.2687e-04 - val_loss: 3.9471e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 97/100\n",
            "3802/3802 - 11s - loss: 7.3496e-05 - val_loss: 3.6882e-04 - 11s/epoch - 3ms/step\n",
            "Epoch 98/100\n",
            "3802/3802 - 10s - loss: 2.1742e-04 - val_loss: 4.8208e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 99/100\n",
            "3802/3802 - 10s - loss: 1.7656e-04 - val_loss: 4.2022e-04 - 10s/epoch - 3ms/step\n",
            "Epoch 100/100\n",
            "3802/3802 - 10s - loss: 1.3446e-04 - val_loss: 0.0010 - 10s/epoch - 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define an encoder model (without the decoder)\n",
        "encoder = Model(inputs=input_data_shape, outputs=bottleneck)\n",
        "# save the encoder to file\n",
        "encoder.save('encoder.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTS7_00AsE81",
        "outputId": "0ad42b2f-5edb-40b1-a71d-d7005e4015c3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "#Compressing the input data using Encoder Model and fitting it on the Logistic Regression model.\n",
        "# load the model from file\n",
        "encoder = load_model('encoder.h5')\n",
        "\n",
        "# encode the train data\n",
        "X_train_encode = encoder.predict(X_train)\n",
        "# encode the test data\n",
        "X_test_encode = encoder.predict(X_test)\n",
        "# define the model\n",
        "model = XGBClassifier(max_iter=100)\n",
        "# fit the model on the training set\n",
        "model.fit(X_train_encode, y_train)\n",
        "# make predictions on the test set\n",
        "yhat = model.predict(X_test_encode)\n",
        "# calculate classification accuracy\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4tpRu0gsYrl",
        "outputId": "2e79c0e0-c453-4356-d447-51e0afb666df"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "0.9999342364855978\n"
          ]
        }
      ]
    }
  ]
}