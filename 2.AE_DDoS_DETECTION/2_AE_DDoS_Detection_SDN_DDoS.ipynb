{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WgrndRzPeMsM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
        "from tensorflow.keras.losses import MeanSquaredError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "yYnLpe-5eW5B",
        "outputId": "89b68ae7-6152-4d6d-b7d4-80a40b282ca0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8ba79755-384c-47de-ae0f-7e89e1840381\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Tot Fwd Pkts</th>\n",
              "      <th>Tot Bwd Pkts</th>\n",
              "      <th>TotLen Fwd Pkts</th>\n",
              "      <th>TotLen Bwd Pkts</th>\n",
              "      <th>Fwd Pkt Len Max</th>\n",
              "      <th>Fwd Pkt Len Min</th>\n",
              "      <th>Fwd Pkt Len Mean</th>\n",
              "      <th>Fwd Pkt Len Std</th>\n",
              "      <th>Bwd Pkt Len Max</th>\n",
              "      <th>Bwd Pkt Len Min</th>\n",
              "      <th>Bwd Pkt Len Mean</th>\n",
              "      <th>Bwd Pkt Len Std</th>\n",
              "      <th>Flow Byts/s</th>\n",
              "      <th>Flow Pkts/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Tot</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Tot</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Bwd URG Flags</th>\n",
              "      <th>Fwd Header Len</th>\n",
              "      <th>Bwd Header Len</th>\n",
              "      <th>Fwd Pkts/s</th>\n",
              "      <th>Bwd Pkts/s</th>\n",
              "      <th>Pkt Len Min</th>\n",
              "      <th>Pkt Len Max</th>\n",
              "      <th>Pkt Len Mean</th>\n",
              "      <th>Pkt Len Std</th>\n",
              "      <th>Pkt Len Var</th>\n",
              "      <th>FIN Flag Cnt</th>\n",
              "      <th>SYN Flag Cnt</th>\n",
              "      <th>RST Flag Cnt</th>\n",
              "      <th>PSH Flag Cnt</th>\n",
              "      <th>ACK Flag Cnt</th>\n",
              "      <th>URG Flag Cnt</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Cnt</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Pkt Size Avg</th>\n",
              "      <th>Fwd Seg Size Avg</th>\n",
              "      <th>Bwd Seg Size Avg</th>\n",
              "      <th>Subflow Fwd Pkts</th>\n",
              "      <th>Subflow Fwd Byts</th>\n",
              "      <th>Subflow Bwd Pkts</th>\n",
              "      <th>Subflow Bwd Byts</th>\n",
              "      <th>Fwd Act Data Pkts</th>\n",
              "      <th>Fwd Seg Size Min</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>245230</td>\n",
              "      <td>44</td>\n",
              "      <td>40</td>\n",
              "      <td>124937</td>\n",
              "      <td>1071</td>\n",
              "      <td>9100</td>\n",
              "      <td>0</td>\n",
              "      <td>2839.477273</td>\n",
              "      <td>1839.508257</td>\n",
              "      <td>517</td>\n",
              "      <td>0</td>\n",
              "      <td>26.775000</td>\n",
              "      <td>109.188026</td>\n",
              "      <td>513835.99070</td>\n",
              "      <td>342.535579</td>\n",
              "      <td>2954.578313</td>\n",
              "      <td>7953.221927</td>\n",
              "      <td>64066</td>\n",
              "      <td>-44</td>\n",
              "      <td>238564</td>\n",
              "      <td>5548.000000</td>\n",
              "      <td>10446.29576</td>\n",
              "      <td>64066</td>\n",
              "      <td>2</td>\n",
              "      <td>245230</td>\n",
              "      <td>6287.948718</td>\n",
              "      <td>12986.46879</td>\n",
              "      <td>79070</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>880</td>\n",
              "      <td>804</td>\n",
              "      <td>179.423398</td>\n",
              "      <td>163.112180</td>\n",
              "      <td>0</td>\n",
              "      <td>9100</td>\n",
              "      <td>1482.447059</td>\n",
              "      <td>1933.268313</td>\n",
              "      <td>3.737526e+06</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1500.095238</td>\n",
              "      <td>2839.477273</td>\n",
              "      <td>26.775000</td>\n",
              "      <td>44</td>\n",
              "      <td>124937</td>\n",
              "      <td>40</td>\n",
              "      <td>1071</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1605449</td>\n",
              "      <td>107</td>\n",
              "      <td>149</td>\n",
              "      <td>1071</td>\n",
              "      <td>439537</td>\n",
              "      <td>517</td>\n",
              "      <td>0</td>\n",
              "      <td>10.009346</td>\n",
              "      <td>67.496680</td>\n",
              "      <td>27300</td>\n",
              "      <td>0</td>\n",
              "      <td>2949.912752</td>\n",
              "      <td>3012.589539</td>\n",
              "      <td>274445.34210</td>\n",
              "      <td>159.456949</td>\n",
              "      <td>6295.878431</td>\n",
              "      <td>56408.330520</td>\n",
              "      <td>859760</td>\n",
              "      <td>-102</td>\n",
              "      <td>1332121</td>\n",
              "      <td>12567.179250</td>\n",
              "      <td>83434.14155</td>\n",
              "      <td>861138</td>\n",
              "      <td>2</td>\n",
              "      <td>1603130</td>\n",
              "      <td>10831.959460</td>\n",
              "      <td>73926.65245</td>\n",
              "      <td>861129</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2140</td>\n",
              "      <td>3004</td>\n",
              "      <td>66.648022</td>\n",
              "      <td>92.808928</td>\n",
              "      <td>0</td>\n",
              "      <td>27300</td>\n",
              "      <td>1714.428016</td>\n",
              "      <td>2713.465917</td>\n",
              "      <td>7.362897e+06</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1721.125000</td>\n",
              "      <td>10.009346</td>\n",
              "      <td>2949.912752</td>\n",
              "      <td>107</td>\n",
              "      <td>1071</td>\n",
              "      <td>149</td>\n",
              "      <td>439537</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>53078</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>66</td>\n",
              "      <td>758</td>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>13.200000</td>\n",
              "      <td>29.516097</td>\n",
              "      <td>638</td>\n",
              "      <td>0</td>\n",
              "      <td>151.600000</td>\n",
              "      <td>276.826299</td>\n",
              "      <td>15524.32269</td>\n",
              "      <td>188.401975</td>\n",
              "      <td>5897.555556</td>\n",
              "      <td>15184.845200</td>\n",
              "      <td>46232</td>\n",
              "      <td>19</td>\n",
              "      <td>50302</td>\n",
              "      <td>12575.500000</td>\n",
              "      <td>22521.87727</td>\n",
              "      <td>46251</td>\n",
              "      <td>67</td>\n",
              "      <td>52962</td>\n",
              "      <td>13240.500000</td>\n",
              "      <td>22052.04405</td>\n",
              "      <td>46258</td>\n",
              "      <td>405</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>124</td>\n",
              "      <td>94.200987</td>\n",
              "      <td>94.200987</td>\n",
              "      <td>0</td>\n",
              "      <td>638</td>\n",
              "      <td>74.909091</td>\n",
              "      <td>190.807471</td>\n",
              "      <td>3.640749e+04</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>82.400000</td>\n",
              "      <td>13.200000</td>\n",
              "      <td>151.600000</td>\n",
              "      <td>5</td>\n",
              "      <td>66</td>\n",
              "      <td>5</td>\n",
              "      <td>758</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6975</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>286.738351</td>\n",
              "      <td>6975.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6975</td>\n",
              "      <td>6975</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>143.369176</td>\n",
              "      <td>143.369176</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>190141</td>\n",
              "      <td>13</td>\n",
              "      <td>16</td>\n",
              "      <td>780</td>\n",
              "      <td>11085</td>\n",
              "      <td>427</td>\n",
              "      <td>0</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>130.042942</td>\n",
              "      <td>2596</td>\n",
              "      <td>0</td>\n",
              "      <td>692.812500</td>\n",
              "      <td>794.157350</td>\n",
              "      <td>62401.06027</td>\n",
              "      <td>152.518394</td>\n",
              "      <td>6790.750000</td>\n",
              "      <td>12933.295910</td>\n",
              "      <td>38521</td>\n",
              "      <td>-54</td>\n",
              "      <td>86882</td>\n",
              "      <td>7240.166667</td>\n",
              "      <td>13050.84163</td>\n",
              "      <td>38805</td>\n",
              "      <td>1</td>\n",
              "      <td>190141</td>\n",
              "      <td>12676.066670</td>\n",
              "      <td>15949.09279</td>\n",
              "      <td>38521</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>260</td>\n",
              "      <td>344</td>\n",
              "      <td>68.370315</td>\n",
              "      <td>84.148080</td>\n",
              "      <td>0</td>\n",
              "      <td>2596</td>\n",
              "      <td>395.500000</td>\n",
              "      <td>661.691706</td>\n",
              "      <td>4.378359e+05</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>409.137931</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>692.812500</td>\n",
              "      <td>13</td>\n",
              "      <td>780</td>\n",
              "      <td>16</td>\n",
              "      <td>11085</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ba79755-384c-47de-ae0f-7e89e1840381')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8ba79755-384c-47de-ae0f-7e89e1840381 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8ba79755-384c-47de-ae0f-7e89e1840381');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  ...  Idle Max  Idle Min  Label\n",
              "0         245230            44            40  ...         0         0      0\n",
              "1        1605449           107           149  ...         0         0      0\n",
              "2          53078             5             5  ...         0         0      0\n",
              "3           6975             1             1  ...         0         0      0\n",
              "4         190141            13            16  ...         0         0      0\n",
              "\n",
              "[5 rows x 67 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# load the dataset\n",
        "data = pd.read_csv('/content/SDN_DDoS_.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pWAM52FbeavO"
      },
      "outputs": [],
      "source": [
        "features = data.drop('Label', axis=1)\n",
        "target = data['Label']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    features, target, test_size=0.2, stratify=target\n",
        ")\n",
        "\n",
        "# use case is novelty detection so use only the normal data\n",
        "# for training\n",
        "train_index = y_train[y_train == 0].index\n",
        "train_data = x_train.loc[train_index]\n",
        "\n",
        "# min max scale the input data\n",
        "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "x_train_scaled = min_max_scaler.fit_transform(train_data.copy())\n",
        "x_test_scaled = min_max_scaler.transform(x_test.copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTb7EPBNegG2",
        "outputId": "91f90552-43fc-45bc-fc01-e659eb987569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "856/856 [==============================] - 5s 4ms/step - loss: 0.0168 - accuracy: 0.5173 - val_loss: 0.0023 - val_accuracy: 0.6800\n",
            "Epoch 2/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0024 - accuracy: 0.7378 - val_loss: 0.0018 - val_accuracy: 0.6672\n",
            "Epoch 3/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0020 - accuracy: 0.7463 - val_loss: 0.0014 - val_accuracy: 0.6849\n",
            "Epoch 4/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0017 - accuracy: 0.7655 - val_loss: 0.0013 - val_accuracy: 0.7137\n",
            "Epoch 5/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0016 - accuracy: 0.7799 - val_loss: 0.0012 - val_accuracy: 0.7054\n",
            "Epoch 6/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0015 - accuracy: 0.7888 - val_loss: 0.0012 - val_accuracy: 0.7431\n",
            "Epoch 7/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0015 - accuracy: 0.8008 - val_loss: 0.0011 - val_accuracy: 0.7118\n",
            "Epoch 8/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0014 - accuracy: 0.8103 - val_loss: 0.0010 - val_accuracy: 0.7442\n",
            "Epoch 9/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0014 - accuracy: 0.8184 - val_loss: 0.0010 - val_accuracy: 0.7906\n",
            "Epoch 10/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0013 - accuracy: 0.8247 - val_loss: 0.0010 - val_accuracy: 0.7841\n",
            "Epoch 11/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0013 - accuracy: 0.8289 - val_loss: 9.8432e-04 - val_accuracy: 0.7838\n",
            "Epoch 12/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0013 - accuracy: 0.8361 - val_loss: 0.0010 - val_accuracy: 0.7836\n",
            "Epoch 13/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0013 - accuracy: 0.8403 - val_loss: 9.8692e-04 - val_accuracy: 0.7838\n",
            "Epoch 14/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0013 - accuracy: 0.8383 - val_loss: 9.4696e-04 - val_accuracy: 0.7810\n",
            "Epoch 15/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0012 - accuracy: 0.8405 - val_loss: 9.7306e-04 - val_accuracy: 0.7870\n",
            "Epoch 16/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0012 - accuracy: 0.8441 - val_loss: 8.6388e-04 - val_accuracy: 0.7869\n",
            "Epoch 17/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0012 - accuracy: 0.8448 - val_loss: 8.8572e-04 - val_accuracy: 0.7848\n",
            "Epoch 18/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8481 - val_loss: 8.6572e-04 - val_accuracy: 0.7828\n",
            "Epoch 19/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8493 - val_loss: 8.4755e-04 - val_accuracy: 0.7840\n",
            "Epoch 20/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8492 - val_loss: 8.6823e-04 - val_accuracy: 0.7843\n",
            "Epoch 21/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8509 - val_loss: 8.4049e-04 - val_accuracy: 0.7834\n",
            "Epoch 22/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8492 - val_loss: 8.3697e-04 - val_accuracy: 0.7837\n",
            "Epoch 23/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8523 - val_loss: 8.2402e-04 - val_accuracy: 0.7835\n",
            "Epoch 24/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8548 - val_loss: 8.2605e-04 - val_accuracy: 0.7850\n",
            "Epoch 25/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8549 - val_loss: 8.3675e-04 - val_accuracy: 0.7869\n",
            "Epoch 26/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8549 - val_loss: 8.1310e-04 - val_accuracy: 0.7905\n",
            "Epoch 27/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8547 - val_loss: 8.3647e-04 - val_accuracy: 0.7842\n",
            "Epoch 28/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8556 - val_loss: 8.1244e-04 - val_accuracy: 0.7839\n",
            "Epoch 29/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8573 - val_loss: 8.0671e-04 - val_accuracy: 0.7868\n",
            "Epoch 30/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8566 - val_loss: 7.9871e-04 - val_accuracy: 0.7868\n",
            "Epoch 31/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8574 - val_loss: 8.0005e-04 - val_accuracy: 0.7839\n",
            "Epoch 32/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8569 - val_loss: 8.1192e-04 - val_accuracy: 0.7842\n",
            "Epoch 33/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8587 - val_loss: 8.1632e-04 - val_accuracy: 0.7862\n",
            "Epoch 34/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8570 - val_loss: 8.2358e-04 - val_accuracy: 0.7867\n",
            "Epoch 35/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8576 - val_loss: 8.1687e-04 - val_accuracy: 0.7834\n",
            "Epoch 36/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8598 - val_loss: 7.9076e-04 - val_accuracy: 0.7846\n",
            "Epoch 37/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8599 - val_loss: 7.8598e-04 - val_accuracy: 0.7850\n",
            "Epoch 38/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8603 - val_loss: 7.8734e-04 - val_accuracy: 0.7836\n",
            "Epoch 39/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8613 - val_loss: 7.9734e-04 - val_accuracy: 0.7870\n",
            "Epoch 40/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8625 - val_loss: 8.1521e-04 - val_accuracy: 0.7845\n",
            "Epoch 41/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8630 - val_loss: 7.7197e-04 - val_accuracy: 0.7867\n",
            "Epoch 42/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8614 - val_loss: 8.2827e-04 - val_accuracy: 0.7909\n",
            "Epoch 43/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8630 - val_loss: 7.8134e-04 - val_accuracy: 0.7877\n",
            "Epoch 44/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.9737e-04 - accuracy: 0.8629 - val_loss: 7.7029e-04 - val_accuracy: 0.7844\n",
            "Epoch 45/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 0.8616 - val_loss: 7.8319e-04 - val_accuracy: 0.7866\n",
            "Epoch 46/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8622 - val_loss: 7.9168e-04 - val_accuracy: 0.7884\n",
            "Epoch 47/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.9555e-04 - accuracy: 0.8649 - val_loss: 7.7928e-04 - val_accuracy: 0.7888\n",
            "Epoch 48/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.9994e-04 - accuracy: 0.8654 - val_loss: 7.9259e-04 - val_accuracy: 0.7884\n",
            "Epoch 49/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.8452e-04 - accuracy: 0.8659 - val_loss: 7.6919e-04 - val_accuracy: 0.7919\n",
            "Epoch 50/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.9020e-04 - accuracy: 0.8661 - val_loss: 7.8042e-04 - val_accuracy: 0.7848\n",
            "Epoch 51/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.8545e-04 - accuracy: 0.8641 - val_loss: 7.7296e-04 - val_accuracy: 0.7850\n",
            "Epoch 52/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.8266e-04 - accuracy: 0.8640 - val_loss: 7.6904e-04 - val_accuracy: 0.7850\n",
            "Epoch 53/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.8813e-04 - accuracy: 0.8657 - val_loss: 7.4727e-04 - val_accuracy: 0.7886\n",
            "Epoch 54/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.8330e-04 - accuracy: 0.8685 - val_loss: 7.5937e-04 - val_accuracy: 0.7850\n",
            "Epoch 55/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.8502e-04 - accuracy: 0.8684 - val_loss: 7.7044e-04 - val_accuracy: 0.7946\n",
            "Epoch 56/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.7042e-04 - accuracy: 0.8702 - val_loss: 7.7849e-04 - val_accuracy: 0.7909\n",
            "Epoch 57/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.7905e-04 - accuracy: 0.8727 - val_loss: 7.5380e-04 - val_accuracy: 0.7854\n",
            "Epoch 58/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.7174e-04 - accuracy: 0.8717 - val_loss: 7.7993e-04 - val_accuracy: 0.7858\n",
            "Epoch 59/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.7293e-04 - accuracy: 0.8679 - val_loss: 7.6199e-04 - val_accuracy: 0.7882\n",
            "Epoch 60/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.6325e-04 - accuracy: 0.8694 - val_loss: 7.6505e-04 - val_accuracy: 0.7882\n",
            "Epoch 61/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.7095e-04 - accuracy: 0.8707 - val_loss: 7.6942e-04 - val_accuracy: 0.7894\n",
            "Epoch 62/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.5881e-04 - accuracy: 0.8681 - val_loss: 7.7561e-04 - val_accuracy: 0.7852\n",
            "Epoch 63/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.6832e-04 - accuracy: 0.8676 - val_loss: 7.6112e-04 - val_accuracy: 0.7886\n",
            "Epoch 64/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.6803e-04 - accuracy: 0.8671 - val_loss: 7.7930e-04 - val_accuracy: 0.7863\n",
            "Epoch 65/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.6395e-04 - accuracy: 0.8681 - val_loss: 7.7584e-04 - val_accuracy: 0.7855\n",
            "Epoch 66/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.7187e-04 - accuracy: 0.8671 - val_loss: 7.4720e-04 - val_accuracy: 0.7905\n",
            "Epoch 67/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.6685e-04 - accuracy: 0.8704 - val_loss: 7.6223e-04 - val_accuracy: 0.7948\n",
            "Epoch 68/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.6177e-04 - accuracy: 0.8688 - val_loss: 7.5492e-04 - val_accuracy: 0.7907\n",
            "Epoch 69/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.4446e-04 - accuracy: 0.8744 - val_loss: 7.6898e-04 - val_accuracy: 0.7950\n",
            "Epoch 70/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.4402e-04 - accuracy: 0.8722 - val_loss: 7.6611e-04 - val_accuracy: 0.7920\n",
            "Epoch 71/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.5906e-04 - accuracy: 0.8724 - val_loss: 7.5952e-04 - val_accuracy: 0.7974\n",
            "Epoch 72/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.5252e-04 - accuracy: 0.8762 - val_loss: 7.4586e-04 - val_accuracy: 0.7872\n",
            "Epoch 73/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.3982e-04 - accuracy: 0.8716 - val_loss: 7.5523e-04 - val_accuracy: 0.7951\n",
            "Epoch 74/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.3419e-04 - accuracy: 0.8766 - val_loss: 7.4860e-04 - val_accuracy: 0.7996\n",
            "Epoch 75/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.4666e-04 - accuracy: 0.8708 - val_loss: 7.6473e-04 - val_accuracy: 0.8007\n",
            "Epoch 76/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.4755e-04 - accuracy: 0.8696 - val_loss: 7.4259e-04 - val_accuracy: 0.7920\n",
            "Epoch 77/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.3870e-04 - accuracy: 0.8735 - val_loss: 7.4849e-04 - val_accuracy: 0.7921\n",
            "Epoch 78/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.4721e-04 - accuracy: 0.8739 - val_loss: 7.6041e-04 - val_accuracy: 0.7893\n",
            "Epoch 79/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.3328e-04 - accuracy: 0.8700 - val_loss: 7.3990e-04 - val_accuracy: 0.7892\n",
            "Epoch 80/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.3171e-04 - accuracy: 0.8723 - val_loss: 7.4647e-04 - val_accuracy: 0.7949\n",
            "Epoch 81/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.4375e-04 - accuracy: 0.8731 - val_loss: 7.3547e-04 - val_accuracy: 0.7909\n",
            "Epoch 82/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.3237e-04 - accuracy: 0.8724 - val_loss: 7.3658e-04 - val_accuracy: 0.7883\n",
            "Epoch 83/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.3574e-04 - accuracy: 0.8705 - val_loss: 7.3959e-04 - val_accuracy: 0.7848\n",
            "Epoch 84/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.3613e-04 - accuracy: 0.8685 - val_loss: 7.5103e-04 - val_accuracy: 0.7854\n",
            "Epoch 85/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.4016e-04 - accuracy: 0.8699 - val_loss: 7.6194e-04 - val_accuracy: 0.7860\n",
            "Epoch 86/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.1987e-04 - accuracy: 0.8688 - val_loss: 7.4255e-04 - val_accuracy: 0.7852\n",
            "Epoch 87/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.3433e-04 - accuracy: 0.8701 - val_loss: 7.4652e-04 - val_accuracy: 0.7857\n",
            "Epoch 88/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.2752e-04 - accuracy: 0.8678 - val_loss: 7.4421e-04 - val_accuracy: 0.7879\n",
            "Epoch 89/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.4233e-04 - accuracy: 0.8687 - val_loss: 7.1957e-04 - val_accuracy: 0.7947\n",
            "Epoch 90/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.2441e-04 - accuracy: 0.8698 - val_loss: 7.4183e-04 - val_accuracy: 0.7879\n",
            "Epoch 91/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.3465e-04 - accuracy: 0.8696 - val_loss: 7.3356e-04 - val_accuracy: 0.7874\n",
            "Epoch 92/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.2807e-04 - accuracy: 0.8688 - val_loss: 7.3015e-04 - val_accuracy: 0.7855\n",
            "Epoch 93/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.3031e-04 - accuracy: 0.8716 - val_loss: 7.3077e-04 - val_accuracy: 0.7855\n",
            "Epoch 94/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.3526e-04 - accuracy: 0.8687 - val_loss: 7.2019e-04 - val_accuracy: 0.7966\n",
            "Epoch 95/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.2642e-04 - accuracy: 0.8691 - val_loss: 7.3439e-04 - val_accuracy: 0.7936\n",
            "Epoch 96/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.2295e-04 - accuracy: 0.8667 - val_loss: 7.5565e-04 - val_accuracy: 0.7850\n",
            "Epoch 97/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.3248e-04 - accuracy: 0.8651 - val_loss: 7.3955e-04 - val_accuracy: 0.7844\n",
            "Epoch 98/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.3180e-04 - accuracy: 0.8665 - val_loss: 7.4670e-04 - val_accuracy: 0.7848\n",
            "Epoch 99/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.2180e-04 - accuracy: 0.8678 - val_loss: 7.3829e-04 - val_accuracy: 0.7841\n",
            "Epoch 100/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.1545e-04 - accuracy: 0.8686 - val_loss: 7.4325e-04 - val_accuracy: 0.7894\n"
          ]
        }
      ],
      "source": [
        "class AutoEncoder(Model):\n",
        "  def __init__(self, output_units, code_size=16):\n",
        "    super().__init__()\n",
        "    self.encoder = Sequential([\n",
        "      Dense(66, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(32, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(16, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(code_size, activation='relu')\n",
        "    ])\n",
        "    self.decoder = Sequential([\n",
        "      Dense(16, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(32, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(66, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(output_units, activation='sigmoid')\n",
        "    ])\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    encoded = self.encoder(inputs)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "  \n",
        "model = AutoEncoder(output_units=x_train_scaled.shape[1])\n",
        "# configurations of model\n",
        "model.compile(loss='mse', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "history = model.fit(\n",
        "    x_train_scaled,\n",
        "    x_train_scaled,\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    validation_data=(x_test_scaled, x_test_scaled)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "x_ZizsW7fduI",
        "outputId": "d4e6da17-9fb2-4102-bd92-952712b02ce4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DrfThARfh90",
        "outputId": "371bcd89-6c38-47a1-a881-89c31afe361d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold: 0.0019437121899367092\n"
          ]
        }
      ],
      "source": [
        "def find_threshold(model, x_train_scaled):\n",
        "  reconstructions = model.predict(x_train_scaled)\n",
        "  # provides losses of individual instances\n",
        "  reconstruction_errors = tf.keras.losses.msle(reconstructions, x_train_scaled)\n",
        "  # threshold for anomaly scores\n",
        "  threshold = np.mean(reconstruction_errors.numpy()) + np.std(reconstruction_errors.numpy())\n",
        "  return threshold\n",
        "\n",
        "def get_predictions(model, x_test_scaled, threshold):\n",
        "  predictions = model.predict(x_test_scaled)\n",
        "  # provides losses of individual instances\n",
        "  errors = tf.keras.losses.msle(predictions, x_test_scaled)\n",
        "  \n",
        "  anomaly_mask = pd.Series(errors) > threshold\n",
        "  preds = anomaly_mask.map(lambda x: 1.0 if x == True else 0.0)\n",
        "  return preds\n",
        "\n",
        "threshold = find_threshold(model, x_train_scaled)\n",
        "print(f\"Threshold: {threshold}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYCEb73WfvwX",
        "outputId": "b8e22059-474c-4b4a-f8fb-5f7d2e9dbc17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8565040115743785"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "predictions = get_predictions(model, x_test_scaled, threshold)\n",
        "accuracy_score(y_test,predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ah4hqUcYOst",
        "outputId": "74f16d5c-813b-436b-e040-07a9692a1025"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.1.0-py3-none-any.whl (98 kB)\n",
            "\u001b[?25l\r\u001b[K     |                            | 10 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |                         | 20 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |                      | 30 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |                  | 40 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |               | 51 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |            | 61 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |        | 71 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |     | 81 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |  | 92 kB 25.6 MB/s eta 0:00:01\r\u001b[K     || 98 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.1.0 kt-legacy-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kerastuner as kt\n",
        "\n",
        "class AutoEncoderTuner(Model):\n",
        "\n",
        "  def __init__(self, hp, output_units, code_size=8):\n",
        "    super().__init__()\n",
        "    dense_1_units = hp.Int('dense_1_units', min_value=16, max_value=72, step=4)\n",
        "    dense_2_units = hp.Int('dense_2_units', min_value=16, max_value=72, step=4)\n",
        "    dense_3_units = hp.Int('dense_3_units', min_value=16, max_value=72, step=4)\n",
        "    dense_4_units = hp.Int('dense_4_units', min_value=16, max_value=72, step=4)\n",
        "    dense_5_units = hp.Int('dense_5_units', min_value=16, max_value=72, step=4)\n",
        "    dense_6_units = hp.Int('dense_6_units', min_value=16, max_value=72, step=4)\n",
        "    \n",
        "    self.encoder = Sequential([\n",
        "      Dense(dense_1_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(dense_2_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(dense_3_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(code_size, activation='relu')\n",
        "    ])\n",
        "    self.decoder = Sequential([\n",
        "      Dense(dense_4_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(dense_5_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(dense_6_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(output_units, activation='sigmoid')\n",
        "    ])\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    encoded = self.encoder(inputs)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "  model = AutoEncoderTuner(hp, 66)\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  model.compile(\n",
        "      loss='msle',\n",
        "      optimizer=Adam(learning_rate=hp_learning_rate),\n",
        "  )\n",
        "  return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoO3KxN1X-Mm",
        "outputId": "db95f2c7-bec0-4640-8f7c-80e94ac804fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_epochs=20,\n",
        "    factor=3,\n",
        "    directory='autoencoder',\n",
        "    project_name='tuning_autoencoder6'\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    x_train_scaled, \n",
        "    x_train_scaled, \n",
        "    epochs=20, \n",
        "    batch_size=64,\n",
        "    validation_data=(x_test_scaled, x_test_scaled)\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnY9ViH_YXQY",
        "outputId": "3531e4e5-fae2-4697-82ea-a7d3c11cc554"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 31 Complete [00h 01m 23s]\n",
            "val_loss: 0.008662069216370583\n",
            "\n",
            "Best val_loss So Far: 0.00047473839367739856\n",
            "Total elapsed time: 00h 16m 10s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = [f'dense_{i}_units' for i in range(1,7)] + ['learning_rate']\n",
        "best_hyperparams = tuner.get_best_hyperparameters()\n",
        "for hps in hparams:\n",
        "  print(f\"{hps}: {best_hyperparams[0][hps]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT34zDeLYvI3",
        "outputId": "2910bceb-86ec-4e40-e05a-efd52a863e86"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dense_1_units: 68\n",
            "dense_2_units: 24\n",
            "dense_3_units: 36\n",
            "dense_4_units: 28\n",
            "dense_5_units: 36\n",
            "dense_6_units: 48\n",
            "learning_rate: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = tuner.get_best_models()[0]\n",
        "best_model.compile(loss='mse',metrics=['accuracy'], optimizer=Adam(0.0001))\n",
        "\n",
        "best_model.fit(\n",
        "    x_train_scaled,\n",
        "    x_train_scaled,\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    validation_data=(x_test_scaled, x_test_scaled)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7fK7zKVY1Og",
        "outputId": "d1f14579-ca73-4dbd-da9a-958cdb97261f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "856/856 [==============================] - 4s 4ms/step - loss: 0.0011 - accuracy: 0.8543 - val_loss: 8.5502e-04 - val_accuracy: 0.7852\n",
            "Epoch 2/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8577 - val_loss: 8.4494e-04 - val_accuracy: 0.7850\n",
            "Epoch 3/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0011 - accuracy: 0.8588 - val_loss: 8.4537e-04 - val_accuracy: 0.7842\n",
            "Epoch 4/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8610 - val_loss: 8.4610e-04 - val_accuracy: 0.7860\n",
            "Epoch 5/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0011 - accuracy: 0.8610 - val_loss: 8.3663e-04 - val_accuracy: 0.7848\n",
            "Epoch 6/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8613 - val_loss: 8.3399e-04 - val_accuracy: 0.7867\n",
            "Epoch 7/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8619 - val_loss: 8.3820e-04 - val_accuracy: 0.7850\n",
            "Epoch 8/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8627 - val_loss: 8.3110e-04 - val_accuracy: 0.7867\n",
            "Epoch 9/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.8624 - val_loss: 8.3278e-04 - val_accuracy: 0.7865\n",
            "Epoch 10/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8631 - val_loss: 8.3248e-04 - val_accuracy: 0.7855\n",
            "Epoch 11/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8624 - val_loss: 8.3383e-04 - val_accuracy: 0.7851\n",
            "Epoch 12/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 0.8620 - val_loss: 8.2573e-04 - val_accuracy: 0.7848\n",
            "Epoch 13/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8620 - val_loss: 8.2308e-04 - val_accuracy: 0.7867\n",
            "Epoch 14/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8623 - val_loss: 8.2712e-04 - val_accuracy: 0.7866\n",
            "Epoch 15/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 0.8617 - val_loss: 8.2397e-04 - val_accuracy: 0.7859\n",
            "Epoch 16/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8621 - val_loss: 8.2775e-04 - val_accuracy: 0.7868\n",
            "Epoch 17/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8631 - val_loss: 8.2711e-04 - val_accuracy: 0.7854\n",
            "Epoch 18/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8625 - val_loss: 8.1583e-04 - val_accuracy: 0.7865\n",
            "Epoch 19/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8614 - val_loss: 8.2157e-04 - val_accuracy: 0.7863\n",
            "Epoch 20/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 0.8621 - val_loss: 8.2673e-04 - val_accuracy: 0.7864\n",
            "Epoch 21/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8621 - val_loss: 8.2519e-04 - val_accuracy: 0.7859\n",
            "Epoch 22/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8619 - val_loss: 8.2088e-04 - val_accuracy: 0.7860\n",
            "Epoch 23/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8614 - val_loss: 8.2254e-04 - val_accuracy: 0.7840\n",
            "Epoch 24/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8623 - val_loss: 8.2536e-04 - val_accuracy: 0.7863\n",
            "Epoch 25/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8617 - val_loss: 8.1860e-04 - val_accuracy: 0.7852\n",
            "Epoch 26/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8623 - val_loss: 8.1793e-04 - val_accuracy: 0.7860\n",
            "Epoch 27/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8614 - val_loss: 8.1939e-04 - val_accuracy: 0.7867\n",
            "Epoch 28/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8615 - val_loss: 8.2288e-04 - val_accuracy: 0.7862\n",
            "Epoch 29/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8627 - val_loss: 8.1999e-04 - val_accuracy: 0.7861\n",
            "Epoch 30/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8607 - val_loss: 8.1669e-04 - val_accuracy: 0.7861\n",
            "Epoch 31/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8614 - val_loss: 8.2116e-04 - val_accuracy: 0.7840\n",
            "Epoch 32/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8619 - val_loss: 8.1760e-04 - val_accuracy: 0.7859\n",
            "Epoch 33/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 0.8630 - val_loss: 8.1943e-04 - val_accuracy: 0.7848\n",
            "Epoch 34/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8617 - val_loss: 8.1273e-04 - val_accuracy: 0.7846\n",
            "Epoch 35/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8631 - val_loss: 8.1202e-04 - val_accuracy: 0.7858\n",
            "Epoch 36/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8630 - val_loss: 8.1043e-04 - val_accuracy: 0.7855\n",
            "Epoch 37/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8622 - val_loss: 8.1577e-04 - val_accuracy: 0.7861\n",
            "Epoch 38/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8624 - val_loss: 8.1259e-04 - val_accuracy: 0.7860\n",
            "Epoch 39/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8623 - val_loss: 8.1509e-04 - val_accuracy: 0.7861\n",
            "Epoch 40/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 0.8621 - val_loss: 8.1410e-04 - val_accuracy: 0.7911\n",
            "Epoch 41/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8623 - val_loss: 8.1638e-04 - val_accuracy: 0.7911\n",
            "Epoch 42/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8626 - val_loss: 8.1129e-04 - val_accuracy: 0.7855\n",
            "Epoch 43/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 0.8611 - val_loss: 8.1239e-04 - val_accuracy: 0.7861\n",
            "Epoch 44/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8612 - val_loss: 8.0934e-04 - val_accuracy: 0.7859\n",
            "Epoch 45/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8638 - val_loss: 8.1503e-04 - val_accuracy: 0.7915\n",
            "Epoch 46/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8621 - val_loss: 8.0953e-04 - val_accuracy: 0.7863\n",
            "Epoch 47/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8623 - val_loss: 8.1117e-04 - val_accuracy: 0.7849\n",
            "Epoch 48/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 0.8629 - val_loss: 8.1058e-04 - val_accuracy: 0.7844\n",
            "Epoch 49/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8623 - val_loss: 8.0725e-04 - val_accuracy: 0.7840\n",
            "Epoch 50/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8621 - val_loss: 8.1178e-04 - val_accuracy: 0.7839\n",
            "Epoch 51/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 0.8613 - val_loss: 8.1199e-04 - val_accuracy: 0.7859\n",
            "Epoch 52/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 0.8626 - val_loss: 8.1015e-04 - val_accuracy: 0.7852\n",
            "Epoch 53/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.9787e-04 - accuracy: 0.8627 - val_loss: 8.0785e-04 - val_accuracy: 0.7850\n",
            "Epoch 54/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 0.8630 - val_loss: 8.1170e-04 - val_accuracy: 0.7854\n",
            "Epoch 55/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8640 - val_loss: 8.1013e-04 - val_accuracy: 0.7850\n",
            "Epoch 56/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 0.8626 - val_loss: 8.1381e-04 - val_accuracy: 0.7846\n",
            "Epoch 57/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8633 - val_loss: 8.0387e-04 - val_accuracy: 0.7861\n",
            "Epoch 58/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8634 - val_loss: 8.1243e-04 - val_accuracy: 0.7909\n",
            "Epoch 59/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 0.8635 - val_loss: 8.1092e-04 - val_accuracy: 0.7847\n",
            "Epoch 60/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 0.8621 - val_loss: 8.0898e-04 - val_accuracy: 0.7842\n",
            "Epoch 61/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 0.8621 - val_loss: 8.0947e-04 - val_accuracy: 0.7916\n",
            "Epoch 62/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.9552e-04 - accuracy: 0.8623 - val_loss: 8.0720e-04 - val_accuracy: 0.7911\n",
            "Epoch 63/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.9886e-04 - accuracy: 0.8639 - val_loss: 8.0542e-04 - val_accuracy: 0.7912\n",
            "Epoch 64/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 0.8628 - val_loss: 8.1167e-04 - val_accuracy: 0.7917\n",
            "Epoch 65/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.9854e-04 - accuracy: 0.8628 - val_loss: 8.0085e-04 - val_accuracy: 0.7860\n",
            "Epoch 66/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8620 - val_loss: 8.0614e-04 - val_accuracy: 0.7909\n",
            "Epoch 67/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8621 - val_loss: 8.0667e-04 - val_accuracy: 0.7909\n",
            "Epoch 68/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8613 - val_loss: 8.0723e-04 - val_accuracy: 0.7840\n",
            "Epoch 69/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.9725e-04 - accuracy: 0.8614 - val_loss: 8.1186e-04 - val_accuracy: 0.7855\n",
            "Epoch 70/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.9541e-04 - accuracy: 0.8632 - val_loss: 8.0714e-04 - val_accuracy: 0.7909\n",
            "Epoch 71/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.9428e-04 - accuracy: 0.8637 - val_loss: 8.0642e-04 - val_accuracy: 0.7915\n",
            "Epoch 72/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.9594e-04 - accuracy: 0.8635 - val_loss: 8.0012e-04 - val_accuracy: 0.7850\n",
            "Epoch 73/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.9223e-04 - accuracy: 0.8640 - val_loss: 8.0316e-04 - val_accuracy: 0.7916\n",
            "Epoch 74/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.9737e-04 - accuracy: 0.8617 - val_loss: 8.0316e-04 - val_accuracy: 0.7917\n",
            "Epoch 75/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.9731e-04 - accuracy: 0.8631 - val_loss: 8.0304e-04 - val_accuracy: 0.7860\n",
            "Epoch 76/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 0.0010 - accuracy: 0.8623 - val_loss: 8.0393e-04 - val_accuracy: 0.7909\n",
            "Epoch 77/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.9506e-04 - accuracy: 0.8617 - val_loss: 8.0591e-04 - val_accuracy: 0.7859\n",
            "Epoch 78/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.8954e-04 - accuracy: 0.8625 - val_loss: 8.0554e-04 - val_accuracy: 0.7907\n",
            "Epoch 79/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.9706e-04 - accuracy: 0.8615 - val_loss: 8.0695e-04 - val_accuracy: 0.7915\n",
            "Epoch 80/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.9426e-04 - accuracy: 0.8629 - val_loss: 8.0351e-04 - val_accuracy: 0.7917\n",
            "Epoch 81/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.9205e-04 - accuracy: 0.8630 - val_loss: 8.0223e-04 - val_accuracy: 0.7916\n",
            "Epoch 82/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.9819e-04 - accuracy: 0.8634 - val_loss: 8.0331e-04 - val_accuracy: 0.7840\n",
            "Epoch 83/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.9292e-04 - accuracy: 0.8626 - val_loss: 8.0959e-04 - val_accuracy: 0.7916\n",
            "Epoch 84/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.8940e-04 - accuracy: 0.8626 - val_loss: 8.0200e-04 - val_accuracy: 0.7859\n",
            "Epoch 85/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.9203e-04 - accuracy: 0.8630 - val_loss: 8.0305e-04 - val_accuracy: 0.7916\n",
            "Epoch 86/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.9264e-04 - accuracy: 0.8631 - val_loss: 7.9959e-04 - val_accuracy: 0.7850\n",
            "Epoch 87/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.8757e-04 - accuracy: 0.8636 - val_loss: 7.9720e-04 - val_accuracy: 0.7909\n",
            "Epoch 88/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.8893e-04 - accuracy: 0.8631 - val_loss: 8.0104e-04 - val_accuracy: 0.7909\n",
            "Epoch 89/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.8704e-04 - accuracy: 0.8644 - val_loss: 7.9879e-04 - val_accuracy: 0.7848\n",
            "Epoch 90/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.8996e-04 - accuracy: 0.8641 - val_loss: 8.0584e-04 - val_accuracy: 0.7911\n",
            "Epoch 91/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.7606e-04 - accuracy: 0.8652 - val_loss: 8.0484e-04 - val_accuracy: 0.7907\n",
            "Epoch 92/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.8898e-04 - accuracy: 0.8632 - val_loss: 7.9863e-04 - val_accuracy: 0.7843\n",
            "Epoch 93/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.8941e-04 - accuracy: 0.8645 - val_loss: 8.0045e-04 - val_accuracy: 0.7910\n",
            "Epoch 94/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.8237e-04 - accuracy: 0.8636 - val_loss: 8.0042e-04 - val_accuracy: 0.7861\n",
            "Epoch 95/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.8939e-04 - accuracy: 0.8642 - val_loss: 8.0163e-04 - val_accuracy: 0.7916\n",
            "Epoch 96/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.8299e-04 - accuracy: 0.8632 - val_loss: 8.0330e-04 - val_accuracy: 0.7908\n",
            "Epoch 97/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.8565e-04 - accuracy: 0.8622 - val_loss: 7.9541e-04 - val_accuracy: 0.7909\n",
            "Epoch 98/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.7922e-04 - accuracy: 0.8631 - val_loss: 7.9815e-04 - val_accuracy: 0.7915\n",
            "Epoch 99/100\n",
            "856/856 [==============================] - 3s 4ms/step - loss: 9.8607e-04 - accuracy: 0.8638 - val_loss: 7.9809e-04 - val_accuracy: 0.7910\n",
            "Epoch 100/100\n",
            "856/856 [==============================] - 3s 3ms/step - loss: 9.8713e-04 - accuracy: 0.8652 - val_loss: 7.9911e-04 - val_accuracy: 0.7904\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9fe09dc650>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold_ = find_threshold(best_model, x_train_scaled)\n",
        "preds_ = get_predictions(best_model, x_test_scaled, threshold_)\n",
        "accuracy_score(preds_, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9Qz00Q0Y_Kb",
        "outputId": "01983dae-529f-4d8e-8f8d-0ed999a42ceb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8567670656319873"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2.AE_DDoS_Detection_SDN_DDoS",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}